{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ace6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('traindata2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13763f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15abe1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 載入 BERT 分詞器\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d3ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  512\n"
     ]
    }
   ],
   "source": [
    "max_len = 512\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9fa904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Tibame_T14\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  請 老大 喜歡 吃 甜食 吃 甜點 老大 心情 忘記 煩惱 同時 控制 亮 發胖 稀有沒有 開箱 20 21年 網路 人氣 甜點 好吃 實至名歸 雷聲 大雨 名過其實 公平 公正 開箱 巴哈 品項 肉桂 捲 肉桂 麵包 害 肉桂 味道 唱 感覺 楓糖 厲害 搭配到 甜 甜味 焦糖 楓糖 味霸肉桂 味道 壓過去 脫 褲子 放屁 買 指數 汽車線 初戀 初戀 國中 弄 二手 麵包店 檸檬塔 441 55 台中 有名 幹掉 名不虛傳 乳酪 味道 檸檬 酸澀感 整體 搭配 聯盟 週年 婚姻 酷教 男人 算 甜 網路 紅 網美 拍照 限動 對 做 拉麵 造型 布丁 對 對 荷包 蛋 醬油 造型 教堂 低進去 容易 拉麵 做 口感 教堂 甜 整體 味道 強調 晚 吃下去 忘記 味道 特別 盒子 僅存 特點 買回家 吃 龍雪 專注 龍雪 穿 選 包裝 好看 文青 掛 不錯 可愛 包裝 雷同區 俱到 意涵 妹 蔓越莓 口味 對 對 家 隔壁 老闆娘 做 拍 死掉 好吃 扎實 紮實 吃進去 熟 奶油 糖霜 溢出來 經驗 講 說 買 好吃 對 對 蔓越莓 奶油 餅乾 搭配 味道 想 撥給 妹 酸 事 香氣 足 熱量 算 頂高 價錢 1881 算 ok 買 買回去 好吃 老大 富 買 指數 星 請 聯絡 條件 薪水 代表 好看 分成 單程 舞落為 濃厚 餅乾 斷輸出 好吃 蛋糕 綿密 衣服 酸味 整體 說 不錯 缺點 強 驚喜感 100 蛋糕 買 200 蛋糕 三峽 中獎 買 買 指數 拿破 放型 彈簧 餡 綿密 棒 港式 飲茶 茶流 沙包 感覺 致命 缺點 病皮 普通 好吃 製 蘋果 好吃 對 對 好吃 外皮 整體 269 長大 老大 工作 時數 7.5 分區 餅乾 曲奇 餅乾 靈魂 綿密度 咬下去 聲 餅乾 吃進去 散掉 解 穠纖 合度 地方 顯色 覺察 鐵 茶 威脅 感覺 講 說 厲害 吃完 嘴巴 顆粒感 厲害 可可曲 甜點 乾乾 乾乾 苦苦 咖啡 豐年 離婚 獨自 孩子 找到 真愛 感覺 甜甜 喜歡 說 生產 可愛 做 品牌 進行式 差 用料 事實 證明 關係 講 話 買 指數 喝下 蛋糕界 愛馬仕 代表 高級 講 開箱 買 買 人體 層次感 不錯 吃 田地 分成 嘴巴 皮 薄 用到 吃進去 吃 感覺 吃 瓦楞紙 感覺 老大 建議 說 女孩子 約會 吃 店 質 感受 氛圍 家 買回家 價值度 高 星 260 喧騰 買 餅乾界 LV 說 文青 宣傳 熟 想 聽說 法國 手工 改許 奶油 做 愛心 奶油 奶油界 高級 好吃 買 包裝 好看 買 指數 教堂 奶油 夾心 餅 吃 打開 奶油 教堂 ok 名字 講 餅乾 脆 香氣度 夠不過 缺點 教堂 強 導致 奶油 奶油 搶走 12 480 40 買來 買 指數 12 爸爸 想 家 高 心 老大 做 2020年 台灣 厲害 夢幻 高 人氣 甜品 評分 都市 負責任 老大 評分 中肯 10 想 租 10 原因 厲害 結果 抱歉 高 9.5 影片 附贈好 商品 連結 喜歡 選購 提供 事情 擔心 吃 吃 胖胖 剩下 減肥 交給 77 老大 分享 訂閱 留言 見偷 愛 交 女朋友 女朋友 開始 狹纔 通人 復行 女孩子 相處 開始 熟悉 感覺 想要 看鎖 解 發現 寬廣 男朋友\n",
      "Token IDs: tensor([  101,  6313,  5439,  1920,  1599,  3631,  1391,  4494,  7608,  1391,\n",
      "         4494,  7953,  5439,  1920,  2552,  2658,  2563,  6250,  4214,  2681,\n",
      "         1398,  3229,  2971,  1169,   778,  4634,  5523,  4921,  3300,  3760,\n",
      "         3300,  7274,  5056,  8113,  8128,  2399,  5206,  6662,   782,  3706,\n",
      "         4494,  7953,  1962,  1391,  2179,  5635,  1399,  3645,  7440,  5476,\n",
      "         1920,  7433,  1399,  6882,  1071,  2179,  1062,  2398,  1062,  3633,\n",
      "         7274,  5056,  2349,  1506,  1501,  7517,  5489,  3424,  2947,  5489,\n",
      "         3424,  7934,  1259,  2154,  5489,  3424,  1456,  6887,  1548,  2697,\n",
      "         6221,  3502,  5131,  1341,  2154,  3022,  6981,  1168,  4494,  4494,\n",
      "         1456,  4193,  5131,  3502,  5131,  1456,  7464,  5489,  3424,  1456,\n",
      "         6887,  1886,  6882,  1343,  5562,  6194,  2094,  3123,  2230,  6525,\n",
      "         2900,  3149,  3749,  6722,  5221,  1159,  2761,  1159,  2761,  1751,\n",
      "          704,  2462,   753,  2797,  7934,  1259,  2421,  3600,  3597,  1849,\n",
      "        13013,  8222,  1378,   704,  3300,  1399,  2402,  2957,  1399,   679,\n",
      "         5995,  1001,   745,  6991,  1456,  6887,  3600,  3597,  7000,  4066,\n",
      "         2697,  3146,  7768,  3022,  6981,  5474,  4673,  6867,  2399,  2042,\n",
      "         2012,  6999,  3136,  4511,   782,  5050,  4494,  5206,  6662,  5148,\n",
      "         5206,  5401,  2864,  4212,  7361,  1240,  2205,   976,  2861,  7934,\n",
      "         6863,  1798,  2357,   672,  2205,  2205,  5792,  1259,  6028,  7016,\n",
      "         3779,  6863,  1798,  3136,  1828,   856,  6868,  1343,  2159,  3211,\n",
      "         2861,  7934,   976,  1366,  2697,  3136,  1828,  4494,  3146,  7768,\n",
      "         1456,  6887,  2485,  6310,  3241,  1391,   678,  1343,  2563,  6250,\n",
      "         1456,  6887,  4294,  1162,  4665,  2094,  1006,  2100,  4294,  7953,\n",
      "         6525,  1726,  2157,  1391,  7983,  7434,  2201,  3800,  7983,  7434,\n",
      "         4959,  6908,  1259,  6172,  1962,  4692,  3152,  7471,  2965,   679,\n",
      "         7097,  1377,  2695,  1259,  6172,  7440,  1398,  1281,   936,  1168,\n",
      "         2692,  3891,  1987,  5913,  6632,  5803,  1366,  1456,  2205,  2205,\n",
      "         2157,  7392,  1880,  5439,  7293,  2023,   976,  2864,  3647,  2957,\n",
      "         1962,  1391,  2799,  2179,  5167,  2179,  1391,  6868,  1343,  4225,\n",
      "         1959,  3779,  5131,  7458,  3980,  1139,   889,  5195,  7710,  6341,\n",
      "         6303,  6525,  1962,  1391,  2205,  2205,  5913,  6632,  5803,  1959,\n",
      "         3779,  7619,   746,  3022,  6981,  1456,  6887,  2682,  3060,  5183,\n",
      "         1987,  7000,   752,  7676,  3706,  6639,  4229,  7030,  5050,  7515,\n",
      "         7770,  1019,  7092,  9460,  8148,  5050,  8270,  6525,  6525,  1726,\n",
      "         1343,  1962,  1391,  5439,  1920,  2168,  6525,  2900,  3149,  3215,\n",
      "         6313,  5474,  5181,  3454,   816,  5959,  3717,   807,  6134,  1962,\n",
      "         4692,  1146,  2768,  1606,  4923,  5659,  5862,  4158,  4083,  1331,\n",
      "         7619,   746,  3174,  6745,  1139,  1962,  1391,  6028,  5130,  5214,\n",
      "         2166,  6132,  3302,  7000,  1456,  3146,  7768,  6303,   679,  7097,\n",
      "         5375,  7953,  2485,  7711,  1599,  2697,  8135,  6028,  5130,  6525,\n",
      "         8185,  6028,  5130,   676,  2296,   704,  4354,  6525,  6525,  2900,\n",
      "         3149,  2897,  4788,  3123,  1798,  2492,  5082,  7630,  5214,  2166,\n",
      "         3472,  3949,  2466,  7614,  5763,  5763,  3837,  3763,  1259,  2697,\n",
      "         6221,  5636,  1462,  5375,  7953,  4567,  4649,  3249,  6858,  1962,\n",
      "         1391,  6182,  5981,  3362,  1962,  1391,  2205,  2205,  1962,  1391,\n",
      "         1912,  4649,  3146,  7768, 11023,  7269,  1920,  5439,  1920,  2339,\n",
      "          868,  3229,  3149,   128,   119,   126,  1146,  1281,  7619,   746,\n",
      "         3289,  1936,  7619,   746,  7470,  7789,  5214,  2166,  2428,  1490,\n",
      "          678,  1343,  5476,  7619,   746,  1391,  6868,  1343,  3141,  2957,\n",
      "         6237,   100,  5270,  1394,  2428,  1765,  3175,  7549,  5682,  6221,\n",
      "         2175,  7136,  5763,  2014,  5545,  2697,  6221,  6341,  6303,  1341,\n",
      "         2154,   102])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in data['text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # 輸入文字\n",
    "                        add_special_tokens = True, # 新增 '[CLS]' 和 '[SEP]'\n",
    "                        max_length = max_len,           # 填充 & 截斷長度\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # 返回 attn. masks.\n",
    "                        return_tensors = 'pt',     # 返回 pytorch tensors 格式的資料\n",
    "                   )\n",
    "    \n",
    "    # 將編碼後的文字加入到列表  \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # 將文字的 attention mask 也加入到 attention_masks 列表\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# 將列表轉換為 tensor\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(data['y'])\n",
    "\n",
    "# 輸出第 1 行文字的原始和編碼後的資訊\n",
    "print('Original: ', data['text'][0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925d4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  262 training samples\n",
      "   29 validation samples\n",
      "  125 testing samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# 將輸入資料合併為 TensorDataset 物件\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 計算訓練集和驗證集大小\n",
    "train_size = int(0.7*0.9 * len(dataset))\n",
    "val_size = int(0.7*0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# 按照資料大小隨機拆分訓練集和測試集\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} testing samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f615491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# 在 fine-tune 的訓練中，BERT 作者建議小批量大小設為 16 或 32\n",
    "batch_size = 32\n",
    "\n",
    "# 為訓練和驗證集建立 Dataloader，對訓練樣本隨機洗牌\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # 訓練樣本\n",
    "            sampler = RandomSampler(train_dataset), # 隨機小批量\n",
    "            batch_size = batch_size # 以小批量進行訓練\n",
    "        )\n",
    "\n",
    "# 驗證集不需要隨機化，這裡順序讀取就好\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # 驗證樣本\n",
    "            sampler = SequentialSampler(val_dataset), # 順序選取小批量\n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec34b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# 載入 BertForSequenceClassification, 預訓練 BERT 模型 + 頂層的線性分類層 \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\", # 小寫的 12 層預訓練模型\n",
    "    num_labels = 4, # 分類數 --2 表示二分類\n",
    "                    # 你可以改變這個數字，用於多分類任務  \n",
    "    output_attentions = False, # 模型是否返回 attentions weights.\n",
    "    output_hidden_states = False, # 模型是否返回所有隱層狀態.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86efd2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (21128, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (4, 768)\n",
      "classifier.bias                                                 (4,)\n"
     ]
    }
   ],
   "source": [
    "# 將所有模型引數轉換為一個列表\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "758a35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3efd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# 訓練 epochs。 BERT 作者建議在 2 和 4 之間，設大了容易過擬合 \n",
    "epochs = 4\n",
    "\n",
    "# 總的訓練樣本數\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 建立學習率排程器\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "745d0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 根據預測結果和標籤資料來計算準確率\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66ca87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # 四捨五入到最近的秒\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # 格式化為 hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3b98696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.45\n",
      "  Training epcoh took: 0:21:53\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.21\n",
      "  Validation Loss: 1.44\n",
      "  Validation took: 0:00:33\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.39\n",
      "  Training epcoh took: 0:19:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.21\n",
      "  Validation Loss: 1.42\n",
      "  Validation took: 0:00:48\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.36\n",
      "  Training epcoh took: 0:19:25\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.21\n",
      "  Validation Loss: 1.43\n",
      "  Validation took: 0:00:36\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.32\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.17\n",
      "  Validation Loss: 1.40\n",
      "  Validation took: 0:00:34\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:22:28 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 以下訓練程式碼是基於 `run_glue.py` 指令碼:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# 設定隨機種子值，以確保輸出是確定的\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 儲存訓練和評估的 loss、準確率、訓練時長等統計指標, \n",
    "training_stats = []\n",
    "\n",
    "# 統計整個訓練時長\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 統計單次 epoch 的訓練時間\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 重置每次 epoch 的訓練總 loss\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # 將模型設定為訓練模式。這裡並不是呼叫訓練介面的意思\n",
    "    # dropout、batchnorm 層在訓練和測試模式下的表現是不同的 (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # 訓練集小批量迭代\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # 每經過40次迭代，就輸出進度資訊\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 準備輸入資料，並將其拷貝到 gpu 中\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # 每次計算梯度前，都需要將梯度清 0，因為 pytorch 的梯度是累加的\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # 前向傳播\n",
    "        # 文件參見: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # 該函式會根據不同的引數，會返回不同的值。 本例中, 會返回 loss 和 logits -- 模型的預測結果\n",
    "        output = model(b_input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)\n",
    "        loss, logits = output[:2]\n",
    "        # 累加 loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # 反向傳播\n",
    "        loss.backward()\n",
    "\n",
    "        # 梯度裁剪，避免出現梯度爆炸情況\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 更新引數\n",
    "        optimizer.step()\n",
    "\n",
    "        # 更新學習率\n",
    "        scheduler.step()\n",
    "\n",
    "    # 平均訓練誤差\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # 單次 epoch 的訓練時長\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # 完成一次 epoch 訓練後，就對該模型的效能進行驗證\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 設定模型為評估模式\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # 將輸入資料載入到 gpu 中\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # 評估的時候不需要更新引數、計算梯度\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels)\n",
    "        loss, logits = output[:2]            \n",
    "        # 累加 loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # 將預測結果和 labels 載入到 cpu 中計算\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # 計算準確率\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # 列印本次 epoch 的準確率\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # 統計本次 epoch 的 loss\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # 統計本次評估的時長\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # 記錄本次 epoch 的所有統計資訊\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97e250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
